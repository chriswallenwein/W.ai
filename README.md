# Neural Networks

Quote: If you can't build it, you don't understand it.

The goal of this repository is to build something like [Googles Tensorflow Playground](https://playground.tensorflow.org/) from scratch using NumPy. The focus lies on the algorithms and not so much on the UI.

## TODO
[x] Forward pass  
[x] Backward pass  
[x] Activation functions  
&nbsp;&nbsp;[x] ReLU  
&nbsp;&nbsp;[ ] Sigmoid  
&nbsp;&nbsp;[ ] Softmax  
&nbsp;&nbsp;[ ] TanH  
[ ] Loss/Cost functions  
&nbsp;&nbsp;[x] L1 loss  
&nbsp;&nbsp;[x] L2 loss  
&nbsp;&nbsp;[ ] Softmax loss  
&nbsp;&nbsp;[ ] Cross-Entropy loss  
[ ] Optimizers  
&nbsp;&nbsp;[ ] Stochastic gradient descent  
&nbsp;&nbsp;[ ] Batch gradient descent  
&nbsp;&nbsp;[ ] Minibatch gradient descent  
&nbsp;&nbsp;[ ] Momentum  
&nbsp;&nbsp;[ ] RMSProp  
&nbsp;&nbsp;[ ] Adam  
[ ] Visualization  
&nbsp;&nbsp;[ ] x: time, y: loss  
[ ] Convolutional Neural Networks  
&nbsp;&nbsp;[ ] convolutional layers  
&nbsp;&nbsp;[ ] pooling layers  
[ ] Regularization  
&nbsp;&nbsp;[ ] L1 regularization  
&nbsp;&nbsp;[ ] L2 regularization  
[ ] Weight initialization  
&nbsp;&nbsp;[ ] Ones  
&nbsp;&nbsp;[ ] Zeros  
&nbsp;&nbsp;[ ] Random Normal initialization  
&nbsp;&nbsp;[ ] Xavier initialization  
[ ] Dropout  
[ ] Normalization  
&nbsp;&nbsp;[ ] Batch norm  
&nbsp;&nbsp;[ ] Layer norm  
&nbsp;&nbsp;[ ] Instance norm  
&nbsp;&nbsp;[ ] Group norm  
[ ] Attention layer  
[ ] simplify code by switching NumPy with JAX  
[ ] Implement traditional machine learning algorithms (regression, KNN, SVM, etc.)  